{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard 可视化工具\n",
    "\n",
    "在数据处理过程中，用户通常想要可以直观查看$\\color{#ea4339}{数据集}$的分布情况。\n",
    "\n",
    "在模型设计过程中，用户需要分析和检查$\\color{#ea4339}{数据流图}$是否正确实现。\n",
    "\n",
    "在模型训练过程中，用户也需要关注$\\color{#ea4339}{模型参数}$和$\\color{#ea4339}{超参数}$变化趋势。\n",
    "\n",
    "在模型测试过程中，用户也要查看$\\color{#ea4339}{准确率}$和$\\color{#ea4339}{召回率}$等评估指标。\n",
    "\n",
    "因此，$TensorFlow$项目组开发了机器学习可视化工具$\\color{#ea4339}{TensorBoard}$,它通过展示直观的图形，能够有效地辅助机器学习程序的开发者和使用者理解算法模型及其工作流程，提升模型开发工作效率。\n",
    "\n",
    "### TensorBoard 可视化训练\n",
    "\n",
    "<img width=80% height=80% src=\"imgs/05/01.png\" alt=\"imgs/05/01.png\" title=\"图1\" />\n",
    "\n",
    "### TensorBoard 可视化统计数据\n",
    "\n",
    "<img width=80% height=80% src=\"imgs/05/02.png\" alt=\"imgs/05/02.png\" title=\"图2\" />\n",
    "\n",
    "### TensorBoard 可视化数据分布\n",
    "\n",
    "<img width=80% height=80% src=\"imgs/05/03.png\" alt=\"imgs/05/03.png\" title=\"图3\" />\n",
    "\n",
    "### TensorBoard 可视化数据集（MNIST）\n",
    "\n",
    "<img width=80% height=80% src=\"imgs/05/04.png\" alt=\"imgs/05/04.png\" title=\"图4\" />\n",
    "\n",
    "### TensorBoard 可视化数据流图\n",
    "\n",
    "<img width=80% height=80% src=\"imgs/05/05.png\" alt=\"imgs/05/05.png\" title=\"图5\" />\n",
    "\n",
    "### TensorBoard 使用流程\n",
    "\n",
    "可视化的数据是数据流图和张量，它们需要在会话中加载或执行操作后才能获取。然后，用户需要使用$\\color{#ea4339}{FileWriter}$实例将这些数据写入事件文件。最后，启动$TensorBoard$程序，加载事件文件中的序列化数据，从而可以在各个面板中展示对应的可视化对象。\n",
    "\n",
    "<img width=80% height=80% src=\"imgs/05/06.png\" alt=\"imgs/05/06.png\" title=\"图6\" />\n",
    "\n",
    "### tf.summary 模块介绍\n",
    "\n",
    "前述流程中使用的FileWriter实例和汇总操作(Summary Ops)均属于tf.summary模块。其主要功能是获取和输出模型相关的序列化数据，它贯通TensorBoard的整个使用流程。\n",
    "\n",
    "tf.summary模块的核心部分由一组汇总操作以及FileWriter、Summary和Event 3个类组成。\n",
    "\n",
    "<img width=60% height=60% src=\"imgs/05/07.png\" alt=\"imgs/05/07.png\" title=\"图7\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化数据流图 工作流\n",
    "\n",
    "<img width=80% height=80% src=\"imgs/05/08.png\" alt=\"imgs/05/08.png\" title=\"图8\" />\n",
    "\n",
    "#### Which one is better ?\n",
    "\n",
    "<img width=80% height=80% src=\"imgs/05/09.png\" alt=\"imgs/05/09.png\" title=\"图9\" />\n",
    "\n",
    "### 名字作用域与抽象节点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy 数据处理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def normalize_feature(df):\n",
    "    return df.apply(lambda column: (column - column.mean()) / column.std())\n",
    "\n",
    "df = normalize_feature(pd.read_csv('datas/04/train_3d.csv', names=['square', 'bedrooms', 'price']))\n",
    "\n",
    "ones = pd.DataFrame({'ones': np.ones(len(df))}) #ones是n行1列的数据框，表示x0恒为1\n",
    "df = pd.concat([ones,df], axis=1) #根据列合并数据\n",
    "\n",
    "X_data = np.array(df[df.columns[0:3]]) #取的0-2列，即前三列；左闭右开的区间\n",
    "y_data = np.array(df[df.columns[-1]]).reshape(len(df), 1) #-1就是最后一列\n",
    "\n",
    "print(X_data.shape, type(X_data))\n",
    "print(y_data.shape, type(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "alpha = 0.01 # 学习率\n",
    "epoch = 400 # 训练全量数据集的轮数\n",
    "\n",
    "# 创建线性回归模型\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    # 输入 X，形状[10000,3]\n",
    "    X = tf.placeholder(tf.float32, X_data.shape)\n",
    "    # 输入 X，形状[10000,1]\n",
    "    y = tf.placeholder(tf.float32, y_data.shape)\n",
    "\n",
    "with tf.name_scope('hypothesis'):\n",
    "    # weight [3, 1]\n",
    "    W = tf.get_variable('weights',(X_data.shape[1], 1), initializer=tf.constant_initializer())\n",
    "    # 假设函数 h(x) = w_0 * x_0 + w_1 * x_1 + w_2 * x_2, 其中x_0恒为1\n",
    "    # 推理值 y_pred [1000,1]\n",
    "    y_pred = tf.matmul(X, W)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    # 损失函数采用最小二乘法，y_pred - y 是形如[1000,1]的向量\n",
    "    # tf.matmul(a, b, transpose_a=True) 表示：矩阵a的转置乘矩阵b，即 [1,1000] x [1000,1]\n",
    "    # 损失函数操作 loss\n",
    "    loss_op = 1 / (2 * len(X_data)) * tf.matmul((y_pred - y), (y_pred - y), transpose_a=True)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    # 随机梯度下降优化器 opt\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=alpha)\n",
    "    \n",
    "    # 单步训练操作 train_op\n",
    "    train_op = opt.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # save sess graph\n",
    "    writer = tf.summary.FileWriter('./summary/linear-regression-5-1', sess.graph)\n",
    "    \n",
    "    for e in range(1, epoch + 1):\n",
    "        sess.run(train_op, feed_dict={X: X_data, y: y_data})\n",
    "        if e % 10 == 0:\n",
    "            loss, w = sess.run([loss_op, W], feed_dict={X: X_data, y: y_data})\n",
    "            log_str = \"Epoch_%d \\t Loss=%.4g \\t Model: y = %.4gx1 + %.4gx2 + %.4g\"\n",
    "            print(log_str % (e, loss, w[1], w[2], w[0]))\n",
    "            \n",
    "# close file writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 启动 TensorBoard\n",
    "\n",
    "<img width=80% height=80% src=\"imgs/05/10.png\" alt=\"imgs/05/10.png\" title=\"图10\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
