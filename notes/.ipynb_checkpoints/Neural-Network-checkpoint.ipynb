{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "[1]:https://cacm.acm.org/news/197529-in-memoriam-marvin-minsky-1927-2016/fulltext\n",
    "[2]:www.baidu.com\n",
    "\n",
    "神经网络是一种模拟人脑的神经网络以期能够实现类人工智能的机器学习技术。人脑中的神经网络是一个非常复杂的组织，成人的大脑中约有1000亿个神经元。\n",
    "\n",
    "<img width=554px height=419px src=\"imgs/brain.jpg\" alt=\"imgs/brain.jpg\" title=\"图1\" />\n",
    "\n",
    "![imgs/brain.jpg](imgs/brainq.jpg \"图1\")\n",
    "\n",
    "机器学习中的神经网络是如何实现这种模拟，并且达到一个惊人的良好效果的？通过本文，你可以得到这些问题的答案，同时还能知道神经网络的历史，以及如何较好地学习它。\n",
    "\n",
    "### 前言\n",
    "\n",
    "让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是输入层，绿色的是输出层，紫色的是中间层（也叫隐藏层）。输入层有3个输入单元，隐藏层有4个单元，输出层有2个单元。后面的文章中，我们统一使用这种颜色来表达神经网络的结构。\n",
    "\n",
    "<img width=35% height=35% src=\"imgs/neural1.jpg\" alt=\"imgs/neural1.jpg\" title=\"图2\" />\n",
    "\n",
    "上面图片可以概括出神经网络的几个特点：\n",
    "- 设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由制定。\n",
    "- 神经网络结构图中的拓扑与箭头代表着预测过程时数据的流向，跟训练时的数据流有一定的区别。\n",
    "- 结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的权重（其值称为权值），这是需要训练得到的。\n",
    "\n",
    "除了从左到右的形式表达的结构图，还有一种常见的表达形式是从下到上来表示一个神经网络。这时候，输入层在图的最下方。输出层则在图的最上方，如下图：\n",
    "\n",
    "<img width=45% height=45% src=\"imgs/neural2.jpg\" alt=\"imgs/neural2.jpg\" title=\"图3\" />\n",
    "\n",
    "从左到右的表达形式以Andrew Ng和LeCun的文献使用较多，Caffe里使用的则是从下到上的表达。在本文中使用从左到右的表达形式。下面从简单的神经元说起，一步一步介绍神经网络复杂结构的形成。\n",
    "\n",
    "### 神经元\n",
    "\n",
    "#### 1 简介\n",
    "\n",
    "对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。\n",
    "\n",
    "一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。\n",
    "\n",
    "人脑中的神经元形状可以用下图做简单的说明：\n",
    "\n",
    "<img width=45% height=45% src=\"imgs/neural3.jpg\" alt=\"imgs/neural3.jpg\" title=\"图4\" />\n",
    "\n",
    "1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP。在下文中，我们会具体介绍神经元模型。\n",
    "\n",
    "| <img width=100% height=100% src=\"imgs/Warren-McCulloch.jpg\" alt=\"imgs/Warren-McCulloch.jpg\" title=\"图5\" /> | <img width=100% height=100% src=\"imgs/Walter-Pitts.jpg\" alt=\"imgs/Walter-Pitts.jpg\" title=\"图6\" /> |\n",
    "| :---- | :---- |\n",
    "| Warren McCulloch | WalterPitts |\n",
    "\n",
    "#### 2 结构\n",
    "\n",
    "神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。\n",
    "\n",
    "下图是一个典型的神经元模型：包含有3个输入，1个输出，以及2个计算功能。\n",
    "\n",
    "注意中间的箭头线。这些线称为“连接”。每个上有一个“权重值”。\n",
    "\n",
    "<img width=45% height=45% src=\"imgs/neural4.jpg\" alt=\"imgs/neural4.jpg\" title=\"图7\" />\n",
    "\n",
    "连接是神经元中最重要的东西。每一个连接上都有一个权重。一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。\n",
    "\n",
    "我们使用$a$来表示输入，用$w$来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是$a$，端中间有加权参数$w$，经过这个加权后的信号会变成$a * w$，因此在连接的末端，信号的大小就变成了$a * w$。\n",
    "\n",
    "<img width=45% height=45% src=\"imgs/neural5.jpg\" alt=\"imgs/neural5.jpg\" title=\"图8\" />\n",
    "\n",
    "在其他绘图模型里，有向箭头可能表示的是值的不变传递。而在神经元模型里，每个有向箭头表示的是值的加权传递。\n",
    "\n",
    "如果我们将神经元图中的所有变量用符号表示，并且写出输出的计算公式的话，就是下图。\n",
    "\n",
    "<img width=50% height=50% src=\"imgs/neural6.jpg\" alt=\"imgs/neural6.jpg\" title=\"图9\" />\n",
    "\n",
    "可见$z$是在输入和权值的线性加权和叠加了一个函数$g$的值。在MP模型里，函数$g$是$sgn$函数，也就是取符号函数。这个函数当输入大于0时，输出1，否则输出0。\n",
    "\n",
    "下面对神经元模型的图进行一些扩展。首先将$sum$函数与$sgn$函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入a与输出$z$写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。\n",
    "\n",
    "神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。\n",
    "\n",
    "<img width=50% height=50% src=\"imgs/neural7.jpg\" alt=\"imgs/neural7.jpg\" title=\"图10\" />\n",
    "\n",
    "当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“单元”（unit）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“节点”（node）来表达同样的意思。\n",
    "\n",
    "#### 3 总结\n",
    "\n",
    "神经元模型的使用可以这样理解：\n",
    "\n",
    "我们有一个数据，称之为样本。样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性预测未知属性。\n",
    "\n",
    "具体办法就是使用神经元的公式进行计算。三个已知属性的值是$a_1$，$a_2$，$a_3$，未知属性的值是$z$。$z$可以通过公式计算出来。这里，已知的属性称之为特征，未知的属性称之为目标。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值$w_1$，$w_2$，$w_3$。那么，我们就可以通过神经元模型预测新样本的目标。\n",
    "\n",
    "#### 4 演进\n",
    "\n",
    "1943年发布的MP模型，虽然简单，但已经建立了神经网络大厦的地基。但是，MP模型中，权重的值都是预先设置的，因此不能学习。\n",
    "\n",
    "1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞的突触（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。\n",
    "\n",
    "| <img width=100% height=100% src=\"imgs/Donald-Olding-Hebb.gif\" alt=\"imgs/Donald-Olding-Hebb.gif\" title=\"图11\" /> |\n",
    "| ---- |\n",
    "| Donald Olding Hebb |\n",
    "\n",
    "尽管神经元模型与Hebb学习律都已诞生，但限于当时的计算机能力，直到接近10年后，第一个真正意义的神经网络才诞生。\n",
    "\n",
    "### 单层神经网络-感知器\n",
    "\n",
    "#### 1 简介\n",
    "\n",
    "1957年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字--“感知器”（Perceptron）（有的文献翻译成“感知机”，下文统一用“感知器”来指代）。\n",
    "\n",
    "感知器是当时首个可以学习的人工神经网络。Rosenblatt现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。\n",
    "\n",
    "人们认为已经发现了智能的奥秘，许多学者和科研机构纷纷投入到神经网络的研究中。美国军方大力资助了神经网络的研究，并认为神经网络比“原子弹工程”更重要。这段时间直到1969年才结束，这个时期可以看作神经网络的第一次高潮。\n",
    "\n",
    "| <img width=100% height=100% src=\"imgs/Frank-Rosenblatt.jpg\" alt=\"imgs/Frank-Rosenblatt.jpg\" title=\"图12\" /> |\n",
    "| :------: |\n",
    "| Frank Rosenblatt |\n",
    "\n",
    "#### 2 结构\n",
    "\n",
    "下面来说明感知器模型。在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变，于是我们就有了下图：从本图开始，我们将权值$w_1$，$w_2$，$w_3$写到“连接线”的中间。\n",
    "\n",
    "<img width=25% height=25% src=\"imgs/neural8.jpg\" alt=\"imgs/neural8.jpg\" title=\"图13\" />\n",
    "\n",
    "在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。\n",
    "\n",
    "我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。但在本文里，我们根据计算层的数量来命名。\n",
    "\n",
    "假如我们要预测的目标不再是一个值，而是一个向量，例如$[2,3]$。那么可以在输出层再增加一个“输出单元”。\n",
    "\n",
    "下图显示了带有两个输出单元的单层神经网络，其中输出单元$z_1$的计算公式如下图。\n",
    "\n",
    "<img width=50% height=50% src=\"imgs/neural9.jpg\" alt=\"imgs/neural9.jpg\" title=\"图14\" />\n",
    "\n",
    "可以看到，$z_1$的计算跟原先的$z$并没有区别。\n",
    "\n",
    "我们已知一个神经元的输出可以向多个神经元传递，因此$z_2$的计算公式如下图。\n",
    "\n",
    "<img width=50% height=50% src=\"imgs/neural10.jpg\" alt=\"imgs/neural10.jpg\" title=\"图15\" />\n",
    "\n",
    "可以看到，$z_2$的计算中除了三个新的权值：$w_4$，$w_5$，$w_6$以外，其他与$z_1$是一样的。\n",
    "\n",
    "整个网络的输出如下图。\n",
    "\n",
    "<img width=50% height=50% src=\"imgs/neural11.jpg\" alt=\"imgs/neural11.jpg\" title=\"图16\" />\n",
    "\n",
    "目前的表达公式有一点不让人满意的就是：$w_4$，$w_5$，$w_6$是后来加的，很难表现出跟原先的$w_1$，$w_2$，$w_3$的关系。\n",
    "\n",
    "因此我们改用二维的下标，用$w_{x,y}$来表达一个权值。下标中的$x$代表后一层神经元的序号，而$y$代表前一层神经元的序号（序号的顺序从上到下）。\n",
    "\n",
    "例如，$w_{1,2}$代表后一层的第1个神经元与前一层的第2个神经元的连接的权值（这种标记方式参照了Andrew Ng的课件）。根据以上方法标记，我们有了下图。\n",
    "\n",
    "<img width=50% height=50% src=\"imgs/neural12.jpg\" alt=\"imgs/neural12.jpg\" title=\"图17\" />\n",
    "\n",
    "如果我们仔细看输出的计算公式，会发现这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。\n",
    "\n",
    "例如，输入的变量是$[a_1，a_2，a_3]$（代表由$a_1，a_2，a_3$组成的列向量），用向量$a$来表示。方程的左边是$[z_1，z_2]$，用向量$z$来表示。系数则是矩阵$W$（2行3列的矩阵，排列形式与公式中的一样）。\n",
    "\n",
    "于是，输出公式可以改写成：\n",
    "\n",
    "$g(W * a) = z;$\n",
    "\n",
    "这个公式就是神经网络中从前一层计算后一层的**矩阵运算**。\n",
    "\n",
    "#### 3 总结\n",
    "\n",
    "与神经元模型不同，感知器中的权值是通过训练得到的。因此，感知器类似一个逻辑回归模型，可以做线性分类任务。\n",
    "\n",
    "我们可以用决策分界来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。\n",
    "\n",
    "下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。\n",
    "\n",
    "<img width=50% height=50% src=\"imgs/neural13.png\" alt=\"imgs/neural13.png\" title=\"图18\" />\n",
    "\n",
    "#### 4 演进\n",
    "\n",
    "感知器只能做简单的线性分类任务。但是当时的人们热情太过于高涨，并没有人清醒的认识到这点。于是，当人工智能领域的巨擘Minsky指出这点时，事态就发生了变化。\n",
    "\n",
    "Minsky在1969年出版了一本叫《Perceptron》的书，里面用详细的数学证明了感知器的弱点，尤其是感知器对$XOR$（异或）这样的简单分类任务都无法解决。\n",
    "\n",
    "Minsky认为，如果将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。（2016年1月，[Minsky][1]在美国去世。谨在本文中纪念这位著名的计算机研究专家与大拿。）\n",
    "\n",
    "| <img width=100% height=100% src=\"imgs/Marvin-Minsky.jpg\" alt=\"imgs/Marvin-Minsky.jpg\" title=\"图19\" /> |\n",
    "| :------: |\n",
    "| Marvin Minsky |\n",
    "\n",
    "由于Minsky的巨大影响力以及书中呈现的悲观态度，让很多学者和实验室纷纷放弃了神经网络的研究。神经网络的研究陷入了冰河期。这个时期又被称为“AI winter”。近10年以后，对于两层神经网络的研究才带来神经网络的复苏。\n",
    "\n",
    "### 两层神经网络-多层感知器\n",
    "\n",
    "#### 1 简介\n",
    "\n",
    "两层神经网络是本文的重点，因为正是在这时候，神经网络开始了大范围的推广与使用。\n",
    "\n",
    "Minsky说过单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。\n",
    "\n",
    "1986年，Rumelhar和Hinton等人提出了**反向传播**（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。 \n",
    "\n",
    "这时候的Hinton还很年轻，30年以后，正是他重新定义了神经网络，带来了神经网络复苏的又一春。\n",
    "\n",
    "| <img width=50% height=50% src=\"imgs/Geoffery-Hinton.jpg\" alt=\"imgs/Geoffery-Hinton.jpg\" title=\"图20\" /> | <img width=70% height=70% src=\"imgs/David-Rumelhart.jpg\" alt=\"imgs/David-Rumelhart.jpg\" title=\"图21\" /> |\n",
    "| :---- | :---- |\n",
    "| Geoffery Hinton | David Rumelhart |\n",
    "\n",
    "#### 2 结构\n",
    "\n",
    "两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。\n",
    "\n",
    "现在，我们的权值矩阵增加到了两个，我们用上标来区分不同层次之间的变量。\n",
    "\n",
    "例如$a_x^{(y)}$代表第$y$层的第$x$个节点。$z_1，z_2$变成了$a_1^{(2)}，a_2^{(2)}$。下图给出了$a_1^{(2)}，a_2^{(2)}$的计算公式。\n",
    "\n",
    "<img width=50% height=50% src=\"imgs/neural14.jpg\" alt=\"imgs/neural14.jpg\" title=\"图21\" />\n",
    "\n",
    "计算最终输出$z$的方式是利用了中间层的$a_1^{(2)}，a_2^{(2)}$和第二个权值矩阵计算得到的，如下图。\n",
    "\n",
    "<img width=50% height=50% src=\"imgs/neural15.jpg\" alt=\"imgs/neural15.jpg\" title=\"图22\" />\n",
    "\n",
    "假设我们的预测目标是一个向量，那么与前面类似，只需要在“输出层”再增加节点即可。\n",
    "\n",
    "我们使用向量和矩阵来表示层次中的变量。$a^{(1)}，a^{(2)}，z$是网络中传输的向量数据。$W^{(1)}和W^{(2)}$是网络的矩阵参数。如下图。\n",
    "\n",
    "<img width=40% height=40% src=\"imgs/neural16.jpg\" alt=\"imgs/neural16.jpg\" title=\"图23\" />\n",
    "\n",
    "使用矩阵运算来表达整个计算公式的话如下：\n",
    "\n",
    "$g(W^{(1)} * a^{(1)}) = a^{(2)}$ \n",
    "\n",
    "$g(W^{(2)} * a^{(2)}) = z$\n",
    "\n",
    "由此可见，使用矩阵运算来表达是很简洁的，而且也不会受到节点数增多的影响（无论有多少节点参与运算，乘法两端都只有一个变量）。因此神经网络的教程中大量使用矩阵运算来描述。\n",
    "\n",
    "需要说明的是，至今为止，我们对神经网络的结构图的讨论中都没有提到**偏置节点（bias unit）**。事实上，这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为 $1$ 的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。\n",
    "\n",
    "偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量$b$，称之为**偏置**。如下图。\n",
    "\n",
    "<img width=40% height=40% src=\"imgs/neural17.jpg\" alt=\"imgs/neural17.jpg\" title=\"图24\" />\n",
    "\n",
    "可以看出，偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）。有些神经网络的结构图中会把偏置节点明显画出来，有些不会。一般情况下，我们都不会明确画出偏置节点。 \n",
    "\n",
    "在考虑了偏置以后的一个神经网络的矩阵运算如下：\n",
    "\n",
    "$g(W^{(1)} * a^{(1)} + b^{(1)}) = a^{(2)}$\n",
    "\n",
    "$g(W^{(2)} * a^{(2)} + b^{(2)}) = z$\n",
    "\n",
    "需要说明的是，在两层神经网络中，我们不再使用$sgn$函数作为函数$g$，而是使用平滑函数 $sigmoid$ 作为函数 $g$。我们把函数 $g$ 也称作**激活函数**（active function）。\n",
    "\n",
    "事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2",
   "language": "python",
   "name": "py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
