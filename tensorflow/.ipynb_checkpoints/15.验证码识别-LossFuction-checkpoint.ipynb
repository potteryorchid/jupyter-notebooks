{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵（Cross-Entropy，CE）\n",
    "\n",
    "我们使用交叉熵作为该模型的损失函数。\n",
    "\n",
    "虽然Categorical／Binary CE是更常用的损失函数，不过他们都是CE的变体。CE定义如下：\n",
    "\n",
    "$$CE=-\\sum_{i}^{C}{t_i \\log(s_i)}$$\n",
    "\n",
    "对于二分类问题$(\\grave{C}=2)$，CE定义如下：\n",
    "\n",
    "$$CE=-\\sum_{i=1}^{\\grave{c}=2}{t_i \\log(s_i)}=-t_1\\log(s_1)+(1-t_1)log(1-s_1)$$\n",
    "\n",
    "### Categorical CE Loss（Softmax Loss）\n",
    "\n",
    "常用于输出为One-hot向量的多类别分类（Multi-Class Classification）模型。\n",
    "\n",
    "<img width=70% height=70% src=\"imgs/15/01.png\" alt=\"imgs/15/01.png\" title=\"图1\" />\n",
    "\n",
    "$$CE=-\\log\\left(\\frac{e^{s_p}}{\\sum_{j}^{C}e^{s_j}}\\right)$$\n",
    "\n",
    "### Binary CE Loss（Sigmoid CE Loss）\n",
    "\n",
    "与Softmax Loss不同，Binary CE Loss对于每个向量分量（class）都是独立的，这意味着每个向量分量计算的$\\color{#CD6600}{损失不受其他分量的影响}$。\n",
    "\n",
    "因此，它常被用于多标签分类（Multi-label classification）模型。\n",
    "\n",
    "<img width=70% height=70% src=\"imgs/15/02.png\" alt=\"imgs/15/02.png\" title=\"图2\" />\n",
    "\n",
    "$$CE= \\begin{cases} -\\log(s_1) & \\text {$if \\quad t_1=1$} \\\\ -log(1-s_1) & \\text{$if \\quad t_1=0$} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
